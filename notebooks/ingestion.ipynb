{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusted Cohort Graduation Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from swampy import structshape as ss\n",
    "import boto3\n",
    "import shelve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_raw_gr_frame(year):\n",
    "    \"\"\"Create raw graduation rate dataframe for a given year from S3. It is 'raw' because\n",
    "    no modifications are done and it is cached locally as-is.\"\"\"\n",
    "    conn = boto3.client('s3')\n",
    "\n",
    "    # Graduation rate raw data filenames on S3\n",
    "    year_to_loc = {2018: \"grad_rate/acgr-sch-sy2018-19-wide.csv\",\n",
    "                   2017: \"grad_rate/acgr-sch-sy2017-18.csv\",\n",
    "                   2016: \"grad_rate/acgr-sch-sy2016-17.csv\",\n",
    "                   2015: \"grad_rate/acgr-sch-sy2015-16.csv\",\n",
    "                   2014: \"grad_rate/acgr-release2-sch-sy2014-15.csv\",\n",
    "                   2013: \"grad_rate/acgr-sch-sy2013-14.csv\",\n",
    "                   2012: \"grad_rate/acgr-sch-sy2012-13.csv\",\n",
    "                   2011: \"grad_rate/acgr-sch-sy2011-12.csv\",\n",
    "                   2010: \"grad_rate/acgr-sch-sy2010-11.csv\"}\n",
    "\n",
    "    # Verify input parameter is valid\n",
    "    if year not in list(range(2010, 2019)):\n",
    "        raise ValueError(\"input parameter {} is out of range.\".format(year))\n",
    "\n",
    "    # Local storage (cache) for the raw data so that iteration time is faster.\n",
    "    shelf = shelve.open(\"gr_dfs\")\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    shelf_key = str(year)\n",
    "\n",
    "    if shelf_key in shelf:\n",
    "        df = shelf[shelf_key]\n",
    "    else:\n",
    "        tmp_file_name = 'curr_file.csv.bak'\n",
    "        conn.download_file('edu-data-bucket', year_to_loc[year], tmp_file_name)\n",
    "\n",
    "        df = pd.read_csv(tmp_file_name)\n",
    "        shelf[shelf_key] = df\n",
    "\n",
    "        if os.path.exists(tmp_file_name):\n",
    "            os.remove(tmp_file_name)\n",
    "\n",
    "    # Shelve teardown\n",
    "    shelf.close()\n",
    "    return df\n",
    "\n",
    "\n",
    "def year_string(y: int):\n",
    "    \"\"\"Input an integer year and get a range that matches the column suffixes in the raw data.\n",
    "    e.g. 2011 => 1112 and 2018 => 1819.\"\"\"\n",
    "    return str(y)[-2:] + str(int(str(y)[-2:]) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [make_raw_gr_frame(year=y) for y in range(2010, 2019)]\n",
    "years = [year_string(y) for y in range(2010, 2019)]\n",
    "\n",
    "# Many of the column names have the school year in them, which presents a challenge for combining all the years\n",
    "# into one large dataframe.\n",
    "\n",
    "\n",
    "shape_data = [(school_year, df.shape) for school_year, df in zip(years, dfs)]\n",
    "shape = pd.DataFrame(shape_data, columns=('school_year', 'shape'))\n",
    "shape\n",
    "\n",
    "# Inspect features that are present in some but not common to all.\n",
    "# Start by removing the years from the column names.\n",
    "# cols_wo_year => Column names without the year\n",
    "cols_wo_year = [list(map(lambda x: x.replace(y, \"\"), df.columns))\n",
    "                for y, df in zip(years, dfs)]\n",
    "print(set(cols_wo_year[3]) - set(cols_wo_year[0]))\n",
    "print(set(cols_wo_year[6]) - set(cols_wo_year[0]))\n",
    "print(set(cols_wo_year[7]) - set(cols_wo_year[0]))\n",
    "print(set(cols_wo_year[8]) - set(cols_wo_year[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT_DATE refers to when the data was inserted and is not relevant for our study.\n",
    "dfs[3].drop(['INSERT_DATE'], axis=1, inplace=True)\n",
    "# ST_SCHID and ST_LEAID are values assigned by the state which are not found in the other years. From the data, it looks like maybe these\n",
    "# started being assigned in 2016. If we need another geographical grouping mechanism in the future we can look into it.\n",
    "dfs[6].drop(['ST_LEAID', 'ST_SCHID'], axis=1, inplace=True)\n",
    "# HOM_COHORT and FCS_COHORT refer to the subpopulation of homeless and foster care students, which was not tracked before school year 2017-2018\n",
    "idx7_sr = shape.school_year[7]\n",
    "idx8_sr = shape.school_year[8]\n",
    "dfs[7].drop(['ST_LEAID', 'ST_SCHID', 'FCS_RATE_'+idx7_sr, 'FCS_COHORT_'+idx7_sr,\n",
    "            'HOM_RATE_'+idx7_sr, 'HOM_COHORT_'+idx7_sr], axis=1, inplace=True)\n",
    "dfs[8].drop(['ST_LEAID', 'ST_SCHID', 'FCS_RATE_'+idx8_sr, 'FCS_COHORT_'+idx8_sr,\n",
    "            'HOM_RATE_'+idx8_sr, 'HOM_COHORT_'+idx8_sr], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create large df of all the years\n",
    "cols_wo_school_year = [list(map(lambda x: x.replace(y, \"\"), df.columns))\n",
    "                       for y, df in zip(years, dfs)]\n",
    "for num1, num2 in zip(range(0, 8), range(1, 9)):\n",
    "    assert cols_wo_school_year[num1] == cols_wo_school_year[num2]\n",
    "\n",
    "big_df = pd.DataFrame()\n",
    "print(\"big_df_columns\", big_df.columns)\n",
    "for idx, df in enumerate(dfs):\n",
    "    df.columns = cols_wo_school_year[0]\n",
    "    df['Year'] = list(range(2010, 2019))[idx]\n",
    "    # reorder columns to be how we want\n",
    "    df = df[['Year']+cols_wo_school_year[0]]\n",
    "    big_df = pd.concat([big_df, df], axis=0)\n",
    "\n",
    "\n",
    "big_df.shape, big_df.head(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missingness\n",
    "big_df.isnull().sum()\n",
    "\n",
    "# Why doesn’t the summation of the major racial and ethnic groups equal the “ALL” student count?\n",
    "# * Due to flexibilities with states’ implementation of the Elementary and Secondary Education Act,\n",
    "#   there may be instances where not all possible groupings of racial/ethnic identification are reported\n",
    "#   as individual subgroups. Therefore, some information may be missing and these counts by major racial\n",
    "#   and ethnic group will not include every student; however any students not included within an individual\n",
    "#   major racial and ethnic group would be included in the “ALL” student count.\n",
    "# #### Why are the major racial and ethnic groups reported differently by states?\n",
    "# * Under the ESEA, a State educational agency (SEA) has the flexibility to determine the major racial/ethnic\n",
    "#   groups it will use for reporting on the data included in its assessment and accountability system.  The\n",
    "#   subgroups that an SEA uses are approved through its Accountability Workbook (the most recent copy of each\n",
    "#   state’s workbook can be found here:  http://www2.ed.gov/admins/lead/account/stateplans03/index.html).\n",
    "#   As a result, there is some variation in how SEAs report data by race and ethnicity.\n",
    "\n",
    "# Drop population subgroup columns\n",
    "cols_to_keep = ['Year', 'STNAM', 'FIPST', 'LEAID', 'LEANM',\n",
    "                'NCESSCH', 'SCHNAM', 'ALL_COHORT_', 'ALL_RATE_']\n",
    "big_df = big_df[cols_to_keep]\n",
    "\n",
    "\n",
    "# Drop rows where ALL_COHORT_ == '.'\n",
    "instances_after_drop = len(big_df) - 2913\n",
    "# Reindex the dataframe because their are duplicate indexes\n",
    "big_df.index = list(range(len(big_df)))\n",
    "big_df.drop(big_df[big_df.ALL_COHORT_ == '.'].index, axis=0, inplace=True)\n",
    "assert len(big_df) == instances_after_drop\n",
    "print(\"Number of instances after this drop\", len(big_df))\n",
    "# Can convert ALL_COHORT_ column to numeric now\n",
    "big_df.loc[:, 'ALL_COHORT_'] = np.int64(big_df.ALL_COHORT_.values)\n",
    "big_df.dtypes\n",
    "\n",
    "# Drop rows where ALL_RATE_ == 'PS'. About 1400 schools per year are dropped.\n",
    "# Approximately 1200 schools per year were dropped that had a cohort size of 5 or less \n",
    "# because they had no graduation rate.\n",
    "# Outliers seen during ingestion of the individual Adjusted Cohort Dataset had cohort sizes of 5-30,000 (3 schools).\n",
    "# These corresponded to one large virtual high school in Ohio and two schools with listed cohort sizes on their website of ~150 (transcription errors).\n",
    "\n",
    "print(\"Dropping\", len(big_df[big_df.ALL_RATE_ == 'PS']))\n",
    "big_df.drop(big_df[big_df.ALL_RATE_ == 'PS'].index, axis=0, inplace=True)\n",
    "print(\"Number of instances after this drop\", len(big_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several columns are of object type, which means that either text or Nan values are present.\n",
    "def print_object_column_info(df):\n",
    "    \"\"\"\n",
    "    Print column data types and the number of object columns.\n",
    "    \"\"\"\n",
    "    print(big_df.dtypes, \"{} object columns present\". format(\n",
    "        len(big_df.select_dtypes(include=object).count())))\n",
    "\n",
    "\n",
    "print_object_column_info(big_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert year feature to numeric\n",
    "big_df.loc[:, 'Year'] = np.int64(big_df.Year)\n",
    "\n",
    "# THe ALL_RATE_ column has numbers given as ranges, numbers with GE,LE,LT,GT prefixes, and also 'PS'.\n",
    "# The codebook explains that these are to conceal specifics when it would give us the ability to identify individual students. The 'PS' specifically means that the cohort had 5 or fewer students in it.\n",
    "# Drop the ALL_RATE_ rows that have 'PS' and convert the rest to the average meaning of the term. E.g. GT50 => 75, LT50 => 25, 88-92 => 90.\n",
    "\n",
    "# Helper functions to convert ranges given in ALL_RATE_ column to a trivially convertible string\n",
    "\n",
    "\n",
    "def conv_range_to_numeric_string(t: str):\n",
    "    \"\"\"Take in a numeric range given as a string, e.g. \"10-20\" and return the midpoint as a string.\"\"\"\n",
    "    vals = t.split(\"-\")\n",
    "    if len(vals) == 2:\n",
    "        val1, val2 = float(vals[0]), float(vals[1])\n",
    "        new_val = val1 / 2.0 + val2 / 2.0\n",
    "        return str(new_val)\n",
    "    else:\n",
    "        # If a range with a dash in it is not found, return original string unchanged.\n",
    "        return t\n",
    "\n",
    "\n",
    "assert conv_range_to_numeric_string('95-95') == '95.0'\n",
    "assert conv_range_to_numeric_string('105-110') == '107.5'\n",
    "assert conv_range_to_numeric_string('90') == '90'\n",
    "\n",
    "\n",
    "def strip_prefixed_string(t: str):\n",
    "    \"\"\"\n",
    "    Convert prefixed numbers to a string of a number in the midpoint of their range \n",
    "    \"\"\"\n",
    "    if t.startswith('GT'):\n",
    "        t = t.removeprefix('GT')\n",
    "        t = float(t)\n",
    "        t = sum([t, 100.0])/2.0\n",
    "        return str(t)\n",
    "    elif t.startswith('GE'):\n",
    "        t = t.removeprefix('GE')\n",
    "        t = float(t)\n",
    "        t = sum([t, 100.0])/2.0\n",
    "        return str(t)\n",
    "    elif t.startswith('LT'):\n",
    "        t = t.removeprefix('LT')\n",
    "        t = float(t)\n",
    "        t = sum([t, 0])/2.0\n",
    "        return str(t)\n",
    "    elif t.startswith('LE'):\n",
    "        t = t.removeprefix('LE')\n",
    "        t = float(t)\n",
    "        t = sum([t, 0])/2.0\n",
    "        return str(t)\n",
    "    # If one of the prefixes is not found, return original string unchanged\n",
    "    return t\n",
    "\n",
    "\n",
    "assert strip_prefixed_string('GE50') == '75.0'\n",
    "assert strip_prefixed_string('GT50') == '75.0'\n",
    "assert strip_prefixed_string('LE90') == '45.0'\n",
    "assert strip_prefixed_string('LT90') == '45.0'\n",
    "assert strip_prefixed_string('95-95') == '95-95'\n",
    "\n",
    "# Run the above functions on the series to produce a float64 series\n",
    "big_df.loc[:, 'ALL_RATE_'] = big_df.ALL_RATE_.map(\n",
    "    strip_prefixed_string, na_action='ignore')\n",
    "big_df.loc[:, 'ALL_RATE_'] = big_df.ALL_RATE_.map(\n",
    "    conv_range_to_numeric_string, na_action='ignore')\n",
    "big_df.loc[:, 'ALL_RATE_'] = pd.to_numeric(big_df.ALL_RATE_.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_school_indexes = big_df.ALL_COHORT_ > 5000\n",
    "big_df.loc[large_school_indexes, :]\n",
    "\n",
    "# Schools are found with >20,000 students in a cohort. Based on google search, the graduating classes are closer to 100-200 than the extremely large sizes reported.\n",
    "# We drop these schools since they are called into question. Note that we could impute them with the mean school size.\n",
    "\n",
    "big_df.drop(big_df[large_school_indexes].index, axis=0, inplace=True)\n",
    "sns.histplot(big_df.ALL_COHORT_,)\n",
    "plt.show()\n",
    "big_df.ALL_COHORT_.describe()\n",
    "\n",
    "sns.histplot(big_df.ALL_COHORT_, cumulative=True, stat='probability')\n",
    "\n",
    "xlimits = np.percentile(big_df.ALL_COHORT_, [5, 95])\n",
    "plt.title(\"Cohort size, x range = 5-95%\")\n",
    "plt.xlim(xlimits)\n",
    "plt.grid(visible=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename BUREAU OF INDIAN AFFAIRS to BUREAU OF INDIAN EDUCATION\n",
    "indian_affairs_rows = big_df.query(\n",
    "    \"STNAM.str.contains('INDIAN AFFAIRS')\").index\n",
    "big_df.loc[indian_affairs_rows, 'STNAM'] = 'BUREAU OF INDIAN EDUCATION'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df.to_csv('grad_rate_intermediate.csv.bak', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# School Directory Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = boto3.client('s3')\n",
    "local_dir_filename = 'directory_all.csv.bak'\n",
    "if not os.path.exists('./' + local_dir_filename):\n",
    "    conn.download_file(\n",
    "        'edu-data-bucket', 'directory/schools_ccd_directory.csv', local_dir_filename)\n",
    "\n",
    "df0 = pd.read_csv(local_dir_filename, low_memory=False)\n",
    "\n",
    "df0.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are many years and schools in this dataset that are not in the graduation rate dataset\n",
    "    * Ncessch and ncessch_num are duplicates\n",
    "    * Features shared with the Adjusted Cohort dataset are\n",
    "        * ncessch\n",
    "        * school_name\n",
    "        * leaid\n",
    "        * lea_name\n",
    "        * state_location\n",
    "        * fips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to our years of interest\n",
    "years = list(range(2010, 2019))\n",
    "our_years = df0.query('year in @years')\n",
    "pd.set_option('display.max_columns', 99)\n",
    "our_years.head(n=3)\n",
    "\n",
    "# Remove uninteresting columns\n",
    "# Uninteresting because duplicate: 'school_id', 'ncessch_num', 'leaid','lea_name','school_name', 'state_location','fips', 'phone_string'\n",
    "# Verify ncessch == ncescch_num since we will merge on it later.\n",
    "# Leave school district identifier for verification that the merge worked later.\n",
    "assert all(our_years.ncessch == our_years.ncessch_num)\n",
    "our_years.drop(['ncessch_num', 'school_id', 'lea_name', 'state_location', 'fips', 'school_status', 'longitude', 'phone', 'city_location', 'street_location', 'street_mailing', 'state_leaid', 'state_leg_district_lower', 'state_leg_district_upper', 'congress_district_id', 'csa', 'cbsa', 'direct_certification', 'title_i_schoolwide', 'lunch_program', 'seasch', 'city_mailing', 'state_mailing', 'elem_cedp', 'middle_cedp', 'high_cedp', 'shared_time', 'bureau_indian_education'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meaningful Missingness\n",
    "# Use school directory cookbook to make decisions are removing columns that we are uncertain about.\n",
    "# Of the schools with missing virtual identifier, we are not able to find any that contain obious names that \n",
    "# would indicate they are virtual. Knowing that virtual schools are less common than brick and mortal, we \n",
    "# impute the missingess as a False value.\n",
    "\n",
    "assert len(our_years.query('virtual < 0 and \"Virtual\" in school_name')) == 0\n",
    "assert len(our_years.query('virtual < 0 and \"virtual\" in school_name')) == 0\n",
    "assert len(our_years.query('virtual < 0 and \"Correspondence\" in school_name')) == 0\n",
    "assert len(our_years.query('virtual < 0 and \"correspondence\" in school_name')) == 0\n",
    "assert len(our_years.query('virtual < 0 and \"Electronic\" in school_name')) == 0\n",
    "assert len(our_years.query('virtual < 0 and \"electronic\" in school_name')) == 0\n",
    "assert len(our_years.query('virtual < 0 and \"digital\" in school_name')) == 0\n",
    "assert len(our_years.query('virtual < 0 and \"Digital\" in school_name')) == 0\n",
    "\n",
    "# This line saves an additional 20K instances and improves the model predictions by about 0.04 accuracy\n",
    "our_years.loc[:, 'virtual'] = our_years.virtual.replace(np.nan, 0)\n",
    "assert our_years.virtual.isnull().sum() == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missingness\n",
    "\n",
    "# Impute -1,-2,-3 missing values to nans\n",
    "# Find strings equal to -1,-2, or -3\n",
    "special_vals = pd.DataFrame([(our_years.select_dtypes(include=[object]) == \"-1\").sum(), (our_years.select_dtypes(\n",
    "    include=[object]) == \"-2\").sum(), (our_years.select_dtypes(include=[object]) == \"-3\").sum()])\n",
    "special_vals.index = [\"-1\", \"-2\", \"-3\"]\n",
    "special_vals\n",
    "\n",
    "special_vals = pd.DataFrame([(our_years.select_dtypes(include=[np.number]) == -1).sum(), (our_years.select_dtypes(\n",
    "    include=[np.number]) == -2).sum(), (our_years.select_dtypes(include=[np.number]) == -3).sum()])\n",
    "special_vals.index = [\"-1\", \"-2\", \"-3\"]\n",
    "special_vals\n",
    "\n",
    "str_cols = our_years.select_dtypes(include=object).columns.tolist()\n",
    "num_cols = our_years.select_dtypes(include=np.number).columns.tolist()\n",
    "str_col_special_nans = (our_years[str_cols] ==\n",
    "                        \"-1\") | (our_years[str_cols] == \"-2\")\n",
    "num_col_special_nans = (our_years[num_cols] < 0)\n",
    "assert \"highest_grade_offered\" in num_cols\n",
    "\n",
    "our_years.loc[:, str_cols] = our_years[str_cols].where(\n",
    "    ~str_col_special_nans, other=np.nan)\n",
    "our_years.loc[:, num_cols] = our_years[num_cols].where(\n",
    "    ~num_col_special_nans, other=np.nan)\n",
    "\n",
    "our_years.highest_grade_offered.value_counts()\n",
    "non_high_schools = our_years.query('highest_grade_offered <= 9')\n",
    "our_years.drop(non_high_schools.index, axis=0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno\n",
    "missingno.matrix(our_years)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine reduced_price_lunch to see how similar it is to free_or_reduced_price_lunch\n",
    "np.subtract(our_years.free_lunch, our_years.free_or_reduced_price_lunch)\n",
    "# It looks like there is some additional information in both columns so we'll keep them both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows with a missing feature, the simplest way to get rid of our nans\n",
    "our_years.dropna(inplace=True)\n",
    "our_years = our_years.convert_dtypes(convert_string=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine features that are still objects\n",
    "pd.DataFrame(our_years.dtypes).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_years.shape\n",
    "missingno.matrix(our_years)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at # FTE Teachers and Enrollment and Free or Reduced Price Lunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrollment less than 1 does not make any sense.\n",
    "# Teachers less than 1 does not make much sense. It is possible that we have part time teachers, but we\n",
    "# are already dropping schools with less than 5 in their graduating cohort. The decision is made to\n",
    "# drop schools with less than 1 teacher as well. There is likley significant overlap between these and the\n",
    "# ALL_COHORT == 'ps' schools.\n",
    "# @NOTE: The schools definitely warrant further study to help understand what is occuring there.\n",
    "# There is over evidence below that the Common Core of Data School Directory dataset is a fairly messy dataset.\n",
    "print(our_years.query('enrollment < 1 or teachers_fte < 1')\n",
    "      ['county_code'].count())\n",
    "print(our_years.query('reduced_price_lunch > enrollment')\n",
    "      ['county_code'].count())\n",
    "print(our_years.query('free_or_reduced_price_lunch > enrollment')\n",
    "      ['county_code'].count())\n",
    "\n",
    "our_years.drop(our_years.query(\n",
    "    'enrollment < 1 or teachers_fte < 1').index, inplace=True)\n",
    "our_years.drop(our_years.query(\n",
    "    'reduced_price_lunch > enrollment').index, inplace=True)\n",
    "our_years.drop(our_years.query(\n",
    "    'free_or_reduced_price_lunch > enrollment').index, inplace=True)\n",
    "\n",
    "print(our_years.query('enrollment < 1 or teachers_fte < 1')\n",
    "      ['county_code'].count())\n",
    "print(our_years.query('reduced_price_lunch > enrollment')\n",
    "      ['county_code'].count())\n",
    "print(our_years.query('free_or_reduced_price_lunch > enrollment')\n",
    "      ['county_code'].count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A small number of schools have a highest_grade_offered less than 12.\n",
    "# Looking up several of these schools\n",
    "# * Hobbs Freshman High School is a 9th grade only school\n",
    "# * Cyberacademy of South Carolina is K-12\n",
    "# * With this mixed information, we are opting to drop the rows.\n",
    "our_years.drop(our_years.query(\n",
    "    'highest_grade_offered < 12').index, inplace=True)\n",
    "\n",
    "\n",
    "print(our_years.teachers_fte.describe())\n",
    "print(our_years.highest_grade_offered.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_years.info()\n",
    "our_years.to_csv('directory_intermediate_dataframe.csv.bak', index=False)\n",
    "our_years.head(n=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math and Reading / Language Arts Assessment Participation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest from S3\n",
    "conn = boto3.client('s3')\n",
    "local_dir_filename = 'math_rla_percent_participation_prefiltered.csv.bak'\n",
    "if not os.path.exists('./' + local_dir_filename):\n",
    "    conn.download_file(\n",
    "        'edu-data-bucket', 'percent_participation/' + local_dir_filename, local_dir_filename)\n",
    "\n",
    "\n",
    "perc_part = pd.read_csv(local_dir_filename, low_memory=False)\n",
    "perc_part.shape, perc_part.info()\n",
    "\n",
    "# Drop rows where Math_Pct_Part == 'PS'.\n",
    "print(\"Dropping\", len(perc_part[perc_part.Math_Pct_Part == 'PS']))\n",
    "print(\"Dropping\", len(perc_part[perc_part.Rla_Pct_Part == 'PS']))\n",
    "perc_part.drop(perc_part[perc_part.Math_Pct_Part ==\n",
    "               '.'].index, axis=0, inplace=True)\n",
    "perc_part.drop(perc_part[perc_part.Rla_Pct_Part ==\n",
    "               '.'].index, axis=0, inplace=True)\n",
    "perc_part.drop(perc_part[perc_part.Math_Pct_Part ==\n",
    "               'PS'].index, axis=0, inplace=True)\n",
    "perc_part.drop(perc_part[perc_part.Rla_Pct_Part ==\n",
    "               'PS'].index, axis=0, inplace=True)\n",
    "print(\"Number of instances after this drop\", len(big_df))\n",
    "\n",
    "# Run the above functions on the series to produce a float64 series\n",
    "perc_part.loc[:, 'Math_Pct_Part'] = perc_part.Math_Pct_Part.map(\n",
    "    strip_prefixed_string, na_action='ignore')\n",
    "perc_part.loc[:, 'Math_Pct_Part'] = perc_part.Math_Pct_Part.map(\n",
    "    conv_range_to_numeric_string, na_action='ignore')\n",
    "perc_part.loc[:, 'Math_Pct_Part'] = pd.to_numeric(\n",
    "    perc_part.Math_Pct_Part.tolist())\n",
    "\n",
    "perc_part.loc[:, 'Rla_Pct_Part'] = perc_part.Rla_Pct_Part.map(\n",
    "    strip_prefixed_string, na_action='ignore')\n",
    "perc_part.loc[:, 'Rla_Pct_Part'] = perc_part.Rla_Pct_Part.map(\n",
    "    conv_range_to_numeric_string, na_action='ignore')\n",
    "perc_part.loc[:, 'Rla_Pct_Part'] = pd.to_numeric(\n",
    "    perc_part.Rla_Pct_Part.tolist())\n",
    "\n",
    "\n",
    "# Save intermediate file locally\n",
    "perc_part.to_csv(\"percent_participation_intermediate.csv.bak\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge datasets and output for exploration and modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Merge intermediate dataframes \n",
    "\"\"\"\n",
    "gr = pd.read_csv('grad_rate_intermediate.csv.bak')\n",
    "directory = pd.read_csv('directory_intermediate_dataframe.csv.bak')\n",
    "perc_part = pd.read_csv('percent_participation_intermediate.csv.bak')\n",
    "\n",
    "perc_part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merge0 = gr.merge(directory, how='left', left_on=(\n",
    "    'Year', 'NCESSCH'), right_on=('year', 'ncessch'))\n",
    "merge1 = merge0.merge(perc_part, how='left', left_on=(\n",
    "    'Year', 'NCESSCH'), right_on=('Year', 'NCESSCH'))\n",
    "merge1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant columns seen after merging\n",
    "merge1.drop(['year', 'ncessch', 'leaid'], axis=1, inplace=True)\n",
    "merge1.dropna(inplace=True)\n",
    "merge1.shape\n",
    "\n",
    "# Reorder columns\n",
    "merge1 = merge1[[\n",
    "    i for i in merge1.columns if i not in 'ALL_RATE_'] + ['ALL_RATE_']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingno.matrix(merge1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View column pairs with minimal positive correlation or above\n",
    "pd.DataFrame((np.abs(merge1.corr()) > 0.3).sum()).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out dataset A to disk\n",
    "merge1.to_csv('../data/mergeA.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# County unemployment rates from USDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For dataset B, we are adding some new data to attempt to capture whatever underlying factors county_code and state ID's were \n",
    "# being selecting for in the first round of modeling.\n",
    "# This way, the model is not predicting performance from past years, and also is not predicting a certain fate based on the county\n",
    "# or state ID itself.\n",
    "# Source: https://www.ers.usda.gov/data-products/county-level-data-sets/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "county_raw = pd.read_excel(\"~/datasets/county_usda_data_2000_2020/Unemployment_Median_Income.xlsx\",header=4)\n",
    "# Un-needed columns at end of the frame\n",
    "county_raw.drop(['Median_Household_Income_2020','Med_HH_Income_Percent_of_State_Total_2020'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\",990)\n",
    "county_raw.tail(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_reshaped = pd.DataFrame(columns=['year','county_code','area_name','rural_urban_continuum_code','urban_influence_code','metro_or_not','civilian_labor_force','unemployment_rate'])\n",
    "for (row_id,series) in county_raw.iterrows():\n",
    "    county_code = series[0]\n",
    "    area_name = series[2]\n",
    "    rural_urban_continuum_code = series[3] \n",
    "    urban_influence_code = series[4]\n",
    "    metro_or_not = series[5]\n",
    "    years = list(range(2000,2022))\n",
    "    id_starts = list(range(6, len(county_raw.columns),4))\n",
    "    for idx, year in enumerate(years):\n",
    "        civilian_labor_force =  series[id_starts[idx]]\n",
    "        unemployment_rate = series[id_starts[idx]+3]\n",
    "        if year < 2010 or year > 2018:\n",
    "            continue\n",
    "        county_reshaped.loc[len(county_reshaped.index)] = year, county_code, area_name, rural_urban_continuum_code,urban_influence_code,metro_or_not,civilian_labor_force,unemployment_rate\n",
    "        # print(year, civilian_labor_force,unemployment_rate)\n",
    "        # Tail and head of the dataset show that these fields are being pulled correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_reshaped.tail(n=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge2 = merge1.merge(county_reshaped,how='left',left_on=['Year','county_code'],right_on=['year','county_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert merge2.Year.equals(merge2.year)\n",
    "merge2.drop('year',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually validate that the school district names appear to match the area names for the counties. \n",
    "# These are from two different datasets so we are making sure the two are self-consitent.\n",
    "merge2[['LEANM','area_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out dataset B to disk\n",
    "merge2.to_csv('../data/mergeB.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('my-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d474be476f0a6db94789ad08b16ede00cb8654a4c727ed769fba79dc07ed6ebd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
